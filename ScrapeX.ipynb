{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from trio~=0.17->selenium) (3.8)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.14 (from trio~=0.17->selenium)\n",
      "  Downloading cffi-1.17.1-cp39-cp39-win_amd64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pycparser (from cffi>=1.14->trio~=0.17->selenium)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.7 MB 9.9 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.2/9.7 MB 15.6 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.5/9.7 MB 17.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.9/9.7 MB 22.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.5/9.7 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.1/9.7 MB 26.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.6/9.7 MB 27.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.7 MB 27.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 25.8 MB/s eta 0:00:00\n",
      "Downloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
      "   ---------------------------------------- 0.0/481.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 481.7/481.7 kB 29.5 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.8/58.8 kB ? eta 0:00:00\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.0/63.0 kB ? eta 0:00:00\n",
      "Downloading cffi-1.17.1-cp39-cp39-win_amd64.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 181.3/181.3 kB 10.7 MB/s eta 0:00:00\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB ? eta 0:00:00\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "   ---------------------------------------- 0.0/117.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 117.6/117.6 kB 7.2 MB/s eta 0:00:00\n",
      "Installing collected packages: sortedcontainers, websocket-client, sniffio, pysocks, pycparser, h11, attrs, wsproto, outcome, cffi, trio, trio-websocket, selenium\n",
      "Successfully installed attrs-24.2.0 cffi-1.17.1 h11-0.14.0 outcome-1.3.0.post0 pycparser-2.22 pysocks-1.7.1 selenium-4.25.0 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.27.0 trio-websocket-0.11.1 websocket-client-1.8.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium beautifulsoup4 pandas openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:WebDriver setup complete.\n",
      "INFO:WDM:====== WebDriver manager ======\n",
      "INFO:WDM:Get LATEST chromedriver version for google-chrome\n",
      "INFO:WDM:Get LATEST chromedriver version for google-chrome\n",
      "INFO:WDM:Driver [C:\\Users\\devan\\.wdm\\drivers\\chromedriver\\win64\\130.0.6723.69\\chromedriver-win32/chromedriver.exe] found in cache\n",
      "INFO:root:Accessed URL: https://fbref.com/en/comps/9/stats/Premier-League-Stats\n",
      "INFO:root:Scrolling down...\n",
      "INFO:root:Scrolling down...\n",
      "INFO:root:Waiting for player stats elements...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "from tkinter import Tk, Label, Entry, Button, filedialog, messagebox, Text, Scrollbar\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Set up logging to identify any issues during scraping\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Configure WebDriver with anti-detection features\n",
    "def setup_webdriver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run browser in headless mode\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--incognito\")  # Use incognito mode for scraping\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36\"\n",
    "    )\n",
    "    logging.info(\"WebDriver setup complete.\")\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Scroll down the page incrementally to trigger all dynamic loads\n",
    "def scroll_to_bottom(driver):\n",
    "    \"\"\"Scrolls to the bottom of the page incrementally.\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        logging.info(\"Scrolling down...\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for content to load\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "# Improved scraping method\n",
    "def scrape_dynamic(url):\n",
    "    \"\"\"Scrapes dynamic content using Selenium WebDriver.\"\"\"\n",
    "    driver = setup_webdriver()\n",
    "    driver.get(url)\n",
    "    logging.info(f\"Accessed URL: {url}\")\n",
    "\n",
    "    # Scroll to the bottom to ensure all content loads\n",
    "    scroll_to_bottom(driver)\n",
    "\n",
    "    try:\n",
    "        # Wait for player stats table to load\n",
    "        logging.info(\"Waiting for player stats elements...\")\n",
    "        players = WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//div[contains(@class, 'player-stats-table')]//ul\"))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        raise ValueError(f\"Could not locate player stats: {str(e)}\")\n",
    "\n",
    "    # Extract data from the elements\n",
    "    data = []\n",
    "    logging.info(\"Extracting player stats...\")\n",
    "    for player in players:\n",
    "        row = [elem.text for elem in player.find_elements(By.TAG_NAME, \"li\")]\n",
    "        if row:\n",
    "            data.append(row)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    if not data:\n",
    "        raise ValueError(\"No relevant data found on the page.\")\n",
    "    \n",
    "    logging.info(f\"Successfully scraped {len(data)} rows.\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Handling the scraping process with a user interface\n",
    "def scrape_data():\n",
    "    \"\"\"Handles the scraping process based on user input.\"\"\"\n",
    "    url = url_entry.get()\n",
    "    try:\n",
    "        # Use Selenium to scrape dynamic content\n",
    "        data = scrape_dynamic(url)\n",
    "\n",
    "        # Display the data in the GUI\n",
    "        text_widget.delete(1.0, 'end')\n",
    "        text_widget.insert('end', data.to_string())\n",
    "\n",
    "        # Save the data as CSV or Excel\n",
    "        def save_file():\n",
    "            file_path = filedialog.asksaveasfilename(\n",
    "                defaultextension=\".csv\",\n",
    "                filetypes=[(\"CSV files\", \"*.csv\"), (\"Excel files\", \"*.xlsx\")]\n",
    "            )\n",
    "            if file_path.endswith('.csv'):\n",
    "                data.to_csv(file_path, index=False)\n",
    "            else:\n",
    "                data.to_excel(file_path, index=False)\n",
    "            messagebox.showinfo(\"Success\", \"Data saved successfully!\")\n",
    "\n",
    "        save_button.config(command=save_file)\n",
    "        save_button.pack(pady=5)\n",
    "\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# GUI Setup\n",
    "app = Tk()\n",
    "app.title(\"Advanced Web Scraper\")\n",
    "\n",
    "# URL Input\n",
    "Label(app, text=\"Enter URL:\").pack()\n",
    "url_entry = Entry(app, width=50)\n",
    "url_entry.pack(pady=5)\n",
    "\n",
    "# Scrape Button\n",
    "Button(app, text=\"Go\", command=scrape_data).pack(pady=5)\n",
    "\n",
    "# Display Scraped Data in a Scrollable Text Widget\n",
    "scrollbar = Scrollbar(app)\n",
    "scrollbar.pack(side='right', fill='y')\n",
    "text_widget = Text(app, wrap='none', yscrollcommand=scrollbar.set)\n",
    "text_widget.pack(expand=True, fill='both')\n",
    "scrollbar.config(command=text_widget.yview)\n",
    "\n",
    "# Save Button (Initially hidden)\n",
    "save_button = Button(app, text=\"Save as CSV/Excel\")\n",
    "\n",
    "# Run the App\n",
    "app.geometry(\"800x600\")\n",
    "app.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from requests->webdriver-manager) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from requests->webdriver-manager) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from requests->webdriver-manager) (2024.8.30)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-1.0.1 webdriver-manager-4.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def setup_webdriver():\n",
    "    \"\"\"Auto-download and set up the latest ChromeDriver.\"\"\"\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tkinter import Tk, Label, Entry, Button, filedialog, messagebox, Text, Scrollbar\n",
    "import time\n",
    "\n",
    "def setup_webdriver():\n",
    "    \"\"\"Sets up Selenium WebDriver with auto-downloaded ChromeDriver.\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "def scrape_static(url):\n",
    "    \"\"\"Scrapes static content using requests and BeautifulSoup.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def scrape_dynamic(url):\n",
    "    \"\"\"Scrapes dynamic content using Selenium WebDriver.\"\"\"\n",
    "    driver = setup_webdriver()\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Wait for the page to fully load\n",
    "    page_content = driver.page_source\n",
    "    driver.quit()\n",
    "    return BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "def extract_tables(soup):\n",
    "    \"\"\"Extracts tables from the BeautifulSoup object.\"\"\"\n",
    "    tables = soup.find_all('table')\n",
    "    if not tables:\n",
    "        raise ValueError(\"No tables found on the page.\")\n",
    "    return pd.read_html(str(tables[0]))[0]  # Convert the first table to DataFrame\n",
    "\n",
    "def scrape_data():\n",
    "    \"\"\"Handles the scraping process based on user input.\"\"\"\n",
    "    url = url_entry.get()\n",
    "    try:\n",
    "        # Determine if the URL needs dynamic scraping\n",
    "        if 'dynamic:' in url:\n",
    "            soup = scrape_dynamic(url.replace('dynamic:', ''))\n",
    "        else:\n",
    "            soup = scrape_static(url)\n",
    "\n",
    "        # Extract and display the data\n",
    "        table_data = extract_tables(soup)\n",
    "        text_widget.delete(1.0, 'end')\n",
    "        text_widget.insert('end', table_data.to_string())\n",
    "\n",
    "        # Save the data as CSV or Excel\n",
    "        def save_file():\n",
    "            file_path = filedialog.asksaveasfilename(\n",
    "                defaultextension=\".csv\",\n",
    "                filetypes=[(\"CSV files\", \"*.csv\"), (\"Excel files\", \"*.xlsx\")]\n",
    "            )\n",
    "            if file_path.endswith('.csv'):\n",
    "                table_data.to_csv(file_path, index=False)\n",
    "            else:\n",
    "                table_data.to_excel(file_path, index=False)\n",
    "            messagebox.showinfo(\"Success\", \"Data saved successfully!\")\n",
    "\n",
    "        save_button.config(command=save_file)\n",
    "        save_button.pack(pady=5)\n",
    "\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", str(e))\n",
    "\n",
    "# GUI Setup\n",
    "app = Tk()\n",
    "app.title(\"Advanced Web Scraper\")\n",
    "\n",
    "# URL Input\n",
    "Label(app, text=\"Enter URL (prefix 'dynamic:' for JavaScript-heavy sites):\").pack()\n",
    "url_entry = Entry(app, width=50)\n",
    "url_entry.pack(pady=5)\n",
    "\n",
    "# Scrape Button\n",
    "Button(app, text=\"Go\", command=scrape_data).pack(pady=5)\n",
    "\n",
    "# Display Scraped Data in a Scrollable Text Widget\n",
    "scrollbar = Scrollbar(app)\n",
    "scrollbar.pack(side='right', fill='y')\n",
    "text_widget = Text(app, wrap='none', yscrollcommand=scrollbar.set)\n",
    "text_widget.pack(expand=True, fill='both')\n",
    "scrollbar.config(command=text_widget.yview)\n",
    "\n",
    "# Save Button (Initially hidden)\n",
    "save_button = Button(app, text=\"Save as CSV/Excel\")\n",
    "\n",
    "# Run the App\n",
    "app.geometry(\"800x600\")\n",
    "app.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 128\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Run the App\u001b[39;00m\n\u001b[0;32m    127\u001b[0m app\u001b[38;5;241m.\u001b[39mgeometry(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m800x600\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 128\u001b[0m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\gba464\\lib\\tkinter\\__init__.py:1429\u001b[0m, in \u001b[0;36mMisc.mainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmainloop\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1429\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tkinter import Tk, Label, Entry, Button, filedialog, messagebox, Text, Scrollbar\n",
    "import time\n",
    "\n",
    "# Setup WebDriver with Options\n",
    "def setup_webdriver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Headless mode for no UI\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")  # Avoid detection\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Scroll Down Method\n",
    "def scroll_to_bottom(driver):\n",
    "    \"\"\"Scroll down the page to load all dynamic content.\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Adjust this delay based on the page's loading speed\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "# Scrape Player Stats using Selenium\n",
    "def scrape_dynamic(url):\n",
    "    \"\"\"Scrapes dynamic content using Selenium WebDriver.\"\"\"\n",
    "    driver = setup_webdriver()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Scroll to the bottom to ensure all content loads\n",
    "    scroll_to_bottom(driver)\n",
    "\n",
    "    # Wait for the player stats table to appear\n",
    "    try:\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//div[contains(@class, 'statistics-detail')]\"))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        raise ValueError(f\"Could not find the player stats table: {str(e)}\")\n",
    "\n",
    "    # Extract the relevant content\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    stats_table = soup.find_all(\"div\", class_=\"statistics-detail__skeleton\")\n",
    "\n",
    "    data = []\n",
    "    for stat in stats_table:\n",
    "        rows = stat.find_all(\"div\", class_=\"skl-table-row\")\n",
    "        for row in rows:\n",
    "            player_data = [cell.get_text(strip=True) for cell in row.find_all(\"div\", class_=\"skl-cell\")]\n",
    "            if player_data:\n",
    "                data.append(player_data)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Convert the data into a DataFrame\n",
    "    if not data:\n",
    "        raise ValueError(\"No relevant data found on the page.\")\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Handle Scraping Process\n",
    "def scrape_data():\n",
    "    \"\"\"Handles the scraping process based on user input.\"\"\"\n",
    "    url = url_entry.get()\n",
    "    try:\n",
    "        # Use Selenium for dynamic scraping\n",
    "        data = scrape_dynamic(url)\n",
    "\n",
    "        # Display the data in the GUI\n",
    "        text_widget.delete(1.0, 'end')\n",
    "        text_widget.insert('end', data.to_string())\n",
    "\n",
    "        # Save Data Function\n",
    "        def save_file():\n",
    "            file_path = filedialog.asksaveasfilename(\n",
    "                defaultextension=\".csv\",\n",
    "                filetypes=[(\"CSV files\", \"*.csv\"), (\"Excel files\", \"*.xlsx\")]\n",
    "            )\n",
    "            if file_path.endswith('.csv'):\n",
    "                data.to_csv(file_path, index=False)\n",
    "            else:\n",
    "                data.to_excel(file_path, index=False)\n",
    "            messagebox.showinfo(\"Success\", \"Data saved successfully!\")\n",
    "\n",
    "        save_button.config(command=save_file)\n",
    "        save_button.pack(pady=5)\n",
    "\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# GUI Setup\n",
    "app = Tk()\n",
    "app.title(\"Advanced Web Scraper\")\n",
    "\n",
    "# URL Input\n",
    "Label(app, text=\"Enter URL:\").pack()\n",
    "url_entry = Entry(app, width=50)\n",
    "url_entry.pack(pady=5)\n",
    "\n",
    "# Scrape Button\n",
    "Button(app, text=\"Go\", command=scrape_data).pack(pady=5)\n",
    "\n",
    "# Display Scraped Data in a Scrollable Text Widget\n",
    "scrollbar = Scrollbar(app)\n",
    "scrollbar.pack(side='right', fill='y')\n",
    "text_widget = Text(app, wrap='none', yscrollcommand=scrollbar.set)\n",
    "text_widget.pack(expand=True, fill='both')\n",
    "scrollbar.config(command=text_widget.yview)\n",
    "\n",
    "# Save Button (Initially hidden)\n",
    "save_button = Button(app, text=\"Save as CSV/Excel\")\n",
    "\n",
    "# Run the App\n",
    "app.geometry(\"800x600\")\n",
    "app.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (4.25.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from selenium) (0.27.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from trio~=0.17->selenium) (3.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\devan\\anaconda3\\envs\\gba464\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tkinter import Tk, Label, Entry, Button, filedialog, messagebox, Text, Scrollbar\n",
    "import time\n",
    "\n",
    "# Configure WebDriver\n",
    "def setup_webdriver():\n",
    "    \"\"\"Sets up Selenium WebDriver with auto-downloaded ChromeDriver.\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run browser in headless mode (no UI)\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\n",
    "        \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36\"\n",
    "    )\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Scroll down the page to load all dynamic content\n",
    "def scroll_to_bottom(driver):\n",
    "    \"\"\"Scrolls down the page to trigger dynamic loading.\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for content to load\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "# Scraping the player stats using Selenium\n",
    "def scrape_dynamic(url):\n",
    "    \"\"\"Scrapes dynamic content using Selenium WebDriver.\"\"\"\n",
    "    driver = setup_webdriver()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Scroll to the bottom to ensure all content loads\n",
    "    scroll_to_bottom(driver)\n",
    "\n",
    "    try:\n",
    "        # Wait until the player stats table is present\n",
    "        players = WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//div[@class='player-stats-table']//ul\"))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        driver.quit()\n",
    "        raise ValueError(f\"Could not find the player stats table: {str(e)}\")\n",
    "\n",
    "    data = []\n",
    "    for player in players:\n",
    "        row = [elem.text for elem in player.find_elements(By.TAG_NAME, \"li\")]\n",
    "        if row:\n",
    "            data.append(row)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    if not data:\n",
    "        raise ValueError(\"No relevant data found on the page.\")\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Handling the scraping process\n",
    "def scrape_data():\n",
    "    \"\"\"Handles the scraping process based on user input.\"\"\"\n",
    "    url = url_entry.get()\n",
    "    try:\n",
    "        # Use Selenium for dynamic scraping\n",
    "        data = scrape_dynamic(url)\n",
    "\n",
    "        # Display the data in the GUI\n",
    "        text_widget.delete(1.0, 'end')\n",
    "        text_widget.insert('end', data.to_string())\n",
    "\n",
    "        # Save the data as CSV or Excel\n",
    "        def save_file():\n",
    "            file_path = filedialog.asksaveasfilename(\n",
    "                defaultextension=\".csv\",\n",
    "                filetypes=[(\"CSV files\", \"*.csv\"), (\"Excel files\", \"*.xlsx\")]\n",
    "            )\n",
    "            if file_path.endswith('.csv'):\n",
    "                data.to_csv(file_path, index=False)\n",
    "            else:\n",
    "                data.to_excel(file_path, index=False)\n",
    "            messagebox.showinfo(\"Success\", \"Data saved successfully!\")\n",
    "\n",
    "        save_button.config(command=save_file)\n",
    "        save_button.pack(pady=5)\n",
    "\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# GUI Setup\n",
    "app = Tk()\n",
    "app.title(\"Advanced Web Scraper\")\n",
    "\n",
    "# URL Input\n",
    "Label(app, text=\"Enter URL:\").pack()\n",
    "url_entry = Entry(app, width=50)\n",
    "url_entry.pack(pady=5)\n",
    "\n",
    "# Scrape Button\n",
    "Button(app, text=\"Go\", command=scrape_data).pack(pady=5)\n",
    "\n",
    "# Display Scraped Data in a Scrollable Text Widget\n",
    "scrollbar = Scrollbar(app)\n",
    "scrollbar.pack(side='right', fill='y')\n",
    "text_widget = Text(app, wrap='none', yscrollcommand=scrollbar.set)\n",
    "text_widget.pack(expand=True, fill='both')\n",
    "scrollbar.config(command=text_widget.yview)\n",
    "\n",
    "# Save Button (Initially hidden)\n",
    "save_button = Button(app, text=\"Save as CSV/Excel\")\n",
    "\n",
    "# Run the App\n",
    "app.geometry(\"800x600\")\n",
    "app.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gba464",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
